class: center, middle

## Processament del Llenguatge Hum√†

# Lab. 2 i 3: Nivell Textual

<br>

### Gerard Escudero, Salvador Medina i Jordi Turmo

## Grau en Intel¬∑lig√®ncia Artificial

<br>

![:scale 75%](../fib.png)

---
class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .cyan[Tokenitzaci√≥]
  
    - Exercici 1: Tokenitzaci√≥

  - Mesures de similaritat

  - Models de llengua

  - Exercici 2: Models de Llengua

  - Zones textuals

- Pr√†ctica 1: Identificaci√≥ d'Idioma

---

# Tokenitzaci√≥ amb NLTK

### Requeriments

```
import nltk
nltk.download('punkt')
```

### Divisi√≥ en frases (*Sentence Splitting*)

```
nltk.sent_tokenize('Men want children. They get relaxed with kids.')

üëâ ['Men want children.', 'They get relaxed with kids.']
```

### Tokenitzador (*Tokenizer*)

```
nltk.word_tokenize('Men want children.')

üëâ ['Men', 'want', 'children', '.']
```

---

# NLTK en castell√†

L'NLTK no t√© models per al catal√†.

```
source = 'El gato tiene hambre. Est√° intentando pescar.'
```

### Divisi√≥ en frases (*Sentence Splitting*)

```
nltk.sent_tokenize(source, language='spanish')

üëâ ['El gato tiene hambre.', 'Est√° intentando pescar.']
```

### Tokenitzador (*Tokenizer*)

```
[nltk.word_tokenize(s, language='spanish') for s in 
 nltk.sent_tokenize(source, language='spanish')]

üëâ [['El', 'gato', 'tiene', 'hambre', '.'], 
    ['Est√°', 'intentando', 'pescar', '.']]
```

---

# Tokenitzaci√≥ amb spaCy (I)

### Requeriments

```
!python -m spacy download ca_core_news_sm
import spacy
nlp = spacy.load('ca_core_news_sm')
```
- Model Angl√®s: `en_core_web_sm`. No cal baixar el model en aquest cas.

- Model castell√†: `es_core_news_sm`

- Model catal√†: `ca_core_news_sm`

- Per tots tres hi ha 4 models: `sm`, `md`, `lg` i `trf`.


### Processament de text

```
source = "L'Arnau t√© un gos. Se l'estima molt."
doc = nlp(source)
```

---

# Tokenitzaci√≥ amb spaCy (II)

### Divisi√≥ en frases (*Sentence Splitting*)

```
[s.text for s in doc.sents]

üëâ ["L'Arnau t√© un gos.", "Se l'estima molt."]
```

### Tokenitzador (*Tokenizer*)

```
s = next(doc.sents)
[(token.text, token.is_stop) for token in s]

üëâ [("L'", False),
    ('Arnau', False),
    ('t√©', False),
    ('un', True),
    ('gos', False),
    ('.', False)]
```

---

# Tokenitzaci√≥ amb TextServer (FreeLing)

### Requeriments

- Script auxiliar: [textserver.py](../codes/textserver.py)

```
from google.colab import drive
import sys

drive.mount('/content/drive')
sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/plh')
from textserver import TextServer
```

### √ös

```
ts = TextServer('usuari', 'passwd', 'tokenizer') 

ts.tokenizer("L'Arnau t√© un gos. Se l'estima molt.")
üëâ  [["L'", 'Arnau', 't√©', 'un', 'gos', '.'], 
     ['Se', "l'", 'estima', 'molt', '.']]
```
---

class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .cyan[Exercici 1: Tokenitzaci√≥]

  - Mesures de similitud

  - Models de llengua

  - Exercici 2: Models de Llengua

  - Zones textuals

- Pr√†ctica 1: Identificaci√≥ d'Idioma


---

# Exercici 1: Tokenitzaci√≥

### Recursos:
Corpus paral¬∑lel del Parlament Europeu.
Aquest corpus inclou transcripcions i traduccions d'intervencions al Parlament Europeu per a diversos idiomes.

```python3
import nltk
# Descarreguem l'Europarl Corpus
nltk.download('europarl_raw')
import nltk.corpus.europarl_raw as europarl
# Obtenim la llista de documents a europarl per l'Angl√®s
europarl.english.fileids()
# Obtenim el text del document 'ep-00-01-17.en'
raw_text = europarl.english.raw('ep-00-01-17.en')
```

### Enunciat
* Tokenitzeu el document 'ep-00-01-17.en' amb els tokenitzadors
descrits en aquesta sessi√≥. 
* Llisteu les difer√®ncies obtingudes (comparant els tokens com a sets).
* Descriviu i analitzeu aquestes difer√®ncies.
* Torneu a realitzar aquest proc√©s pel document `europarl.spanish.raw('ep-00-01-17.es')`, 
sense canviar els tokenitzadors (Angl√®s) i amb tokeni‚à´tzadors pel Castell√† de Spacy i TextServer.
* Compareu els resultats.

---
class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .brown[Exercici 1: Tokenitzaci√≥]

  - .cyan[Mesures de similaritat]

  - Models de llengua

  - Exercici 2: Models de Llengua

  - Zones textuals

- Pr√†ctica 1: Identificaci√≥ d'Idioma

---

# Similaritats

Mesures orientades a conjunts de paraules:

.cols5050[
.col1[
$S_{dice}(X,Y)=\frac{2\cdot \vert X \cap Y\vert}{\vert X\vert+\vert Y\vert}$

$S_{jaccard}(X,Y)=\frac{\vert X \cap Y\vert}{\vert X \cup Y\vert}$
]
.col2[
$S_{overlap}(X,Y)=\frac{\vert X \cap Y\vert}{min(\vert X\vert,\vert Y\vert)}$

$S_{cosine}(X,Y)=\frac{\vert X \cap Y\vert}{\sqrt{\vert X\vert\cdot\vert Y\vert}}$
]]

#### Nota: 

$S_{*}\in[0, 1]$ $\rightarrow$ $D = 1 ‚àí S$.

#### Exemple: 

```
from nltk.metrics import jaccard_distance

jaccard_distance(set(['The','eats','fish','.']),
                 set(['The','eats','blue','fish','.']))
üëâ  0.2
```

---
class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .brown[Exercici 1: Tokenitzaci√≥]

  - .brown[Mesures de similaritat]

  - .cyan[Models de llengua]

  - Exercici 2: Models de Llengua

  - Zones textuals

- Pr√†ctica 1: Identificaci√≥ d'Idioma

---

# n-grames amb nltk I

#### Bigrames de car√†cters:

```
from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder
```

```
finder = BigramCollocationFinder.from_words('a cat')
[tr for tr in finder.ngram_fd.items()]
üëâ  
[(('a', ' '), 1), ((' ', 'c'), 1), (('c', 'a'), 1), (('a', 't'), 1)]
```

#### Trigrames de car√†cters:

```
finder = BigramCollocationFinder.from_words('a cat')
[tr for tr in finder.ngram_fd.items()]
üëâ  [(('a', ' ', 'c'), 1), ((' ', 'c', 'a'), 1), (('c', 'a', 't'), 1)]
```

---

# n-grames amb nltk II

#### n-grames amb filtre:

```
sq = 'the cat and the dog of the man are quite'
finder = TrigramCollocationFinder.from_words(sq)
finder.apply_freq_filter(2)
[tr for tr in finder.ngram_fd.items()]
üëâ  [(('h', 'e', ' '), 3), ((' ', 't', 'h'), 2), (('t', 'h', 'e'), 3)]
```

#### n-grames de paraules:

```
from nltk import word_tokenize
```
```
finder = BigramCollocationFinder.from_words(word_tokenize('the cat and the dog'))
[tr for tr in finder.ngram_fd.items()]
üëâ  
[(('the', 'cat'), 1),
 (('cat', 'and'), 1),
 (('and', 'the'), 1),
 (('the', 'dog'), 1)]
```

---
class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .brown[Exercici 1: Tokenitzaci√≥]

  - .brown[Mesures de similaritat]

  - .brown[Models de llengua]

  - .cyan[Exercici 2: Models de Llengua]

  - Zones textuals

- Pr√†ctica 1: Identificaci√≥ d'Idioma

---

# Exercici 2: Models de Llengua

### Recursos
Com a l'Exercici 1, Corpus paral¬∑lel del Paralment Europeu.

### Enunciat:
* Calculeu els 10 2-grams m√©s freq√ºents (ordenats) per a l'arxiu `europarl.english.raw('ep-00-01-17.en')`.
* Per cada un dels arxius en Angl√®s (`europarl.english.fileids()`), calculeu el percentatge d'aquests 10 2-grams 
que tamb√© es troben en cadascun d'aquests arxius.
* Feu el mateix per a l'arxiu `'ep-00-01-18'`, en aquest cas pels diferents idiomes:
danish, dutch, english, finnish, french, german, greek, italian, portuguese, spanish i swedish.
* √âs suficient aquest model per identificar l'idioma Angl√®s?
* Qu√® passa si fem servir 1-grams? I 5-grams?
---

class: left, middle, inverse

# Sumari

- .cyan[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .brown[Exercici 1: Tokenitzaci√≥]

  - .brown[Mesures de similaritat]

  - .brown[Models de llengua]

  - .brown[Exercici 2: Models de Llengua]

  - .cyan[Zones textuals]

- Pr√†ctica 1: Identificaci√≥ d'Idioma

---

# Beautiful Soup (html)

```Python3
import urllib.request
from bs4 import BeautifulSoup
import re

url = 'https://nlp.lsi.upc.edu/freeling/node/1'
with urllib.request.urlopen(url) as response:
   dt = response.read().decode('utf8')

soup = BeautifulSoup(dt, 'html.parser')
text = re.sub(r'\n+', r'\n', soup.get_text())
print(text)
```
üëâ
```
Welcome | FreeLing Home Page
      Skip to main content
    
User account menu
Show ‚Äî User account menu
Hide ‚Äî User account menu
Log in
FreeLing Home Page
Hooked on a FreeLing
Welcome
Here you can find information about FreeLing, an open source language analysis tool suite, 
...
```

---

# XML 

* Beautiful Soup: `soup = BeautifulSoup(dt, ‚Äôxml‚Äô)`

* [xml.sax](https://docs.python.org/3.7/library/xml.sax.html)

```
import xml.sax

class ChgHandler(xml.sax.ContentHandler):
    cnt = 1
    mn = (1.0, 'EUR')
    
    def startElement(self, name, attrs):
        if name == "Cube":
            if 'rate' in attrs.keys():
                # print(attrs.getValue('currency'), attrs.getValue('rate'))
                ChgHandler.cnt += 1
                ChgHandler.mn = min(ChgHandler.mn,
                                    (float(attrs.getValue('rate')),
                                     attrs.getValue('currency')))

url = 'http://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml'

parser = xml.sax.make_parser()
parser.setContentHandler(ChgHandler())
parser.parse(url)

ChgHandler.mn  üëâ  (0.86408, 'GBP')
```

---
class: left, middle, inverse

# Sumari

- .brown[Sessi√≥]

  - .brown[Tokenitzaci√≥]
  
  - .brown[Exercici 1: Tokenitzaci√≥]

  - .brown[Mesures de similaritat]

  - .brown[Models de llengua]

  - .brown[Exercici 2: Models de Llengua]

  - .brown[Zones textuals]

- .cyan[Pr√†ctica 1: Identificaci√≥ d'Idioma]

---

# Identificaci√≥ d'idioma (guia)

* Preproc√®s:

  - Eliminar els digits del text

  - Convertir tot el text a min√∫scula

  - Substitueix els espais en blanc continus per un de sol

  - Concatena totes les frases amb un espai doble al mig

* Utilitzeu trigrams de car√†cters com a model del llenguatge

* Afegiu una t√®cnica de suavitzat

* Elimineu tots el trigrams que apareguin menys de 5 vegades en el corpus

* Nota: les llibreries *string* i *regular expression* de python us poden ser √∫tils

---

# Identificaci√≥ d'idioma (pr√†ctica 1)

#### Recursos

* [langId.zip](resources/langId.zip)

#### Enunciat

* Implementeu un identificador d'idioma per les lleng√ºes europees: 

  - angl√®s, castell√†, neerland√®s, alemany, itali√† i franc√®s

* Utilitzeu com a dades el [wortschatz leipzig corpora](http://wortschatz.uni-leipzig.de/en/download): <br>
  - 30k frases de cadascuna de les 6 lleng√ºes com a *training set*
  - 10k de cadascuna com a *test set*

* Utilitzeu la guia de la transpar√®ncia anterior i els apunts de teoria i/o el [cap√≠tol 4 del llibre de Jurasky](https://web.stanford.edu/~jurafsky/slp3/4.pdf) de la bibliografia

* Doneu la precisi√≥ (*accuracy*) i la matriu de confusi√≥

* Analitzeu els resultats





