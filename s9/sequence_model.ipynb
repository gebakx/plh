{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Text Similarity\n",
    "Este modelo utiliza gensim para convertir pares de vectores + puntuaciones en vectores (word embeddings).\n",
    "Dado un dataset, infiere la puntuación de similitud entre ambas frases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Requisitos\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:39:15.714539Z",
     "start_time": "2023-05-24T12:39:14.510628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Tipado\n",
    "from typing import Tuple, List, Optional"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:39:15.716990Z",
     "start_time": "2023-05-24T12:39:15.715653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Modelos pre-entrenados\n",
    "WV_MODEL_PATH = \"/Users/salva/Downloads/cc.ca.300.bin.gz\"\n",
    "# from gensim.models import Word2Vec\n",
    "# wv_model = Word2Vec.load('path_to_word2vec_model').wv\n",
    "# from gensim.models import fasttext\n",
    "# wv_model = fasttext.load_facebook_vectors(WV_MODEL_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T07:41:52.327621Z",
     "start_time": "2023-05-24T07:41:52.320007Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Llavors podeu carregar el model com a mmap\n",
    "from gensim.models.fasttext import FastTextKeyedVectors\n",
    "wv_model = FastTextKeyedVectors.load('cc.ca.gensim.bin', mmap='r')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:37.942653Z",
     "start_time": "2023-05-24T12:39:17.092994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Ejemplo de 10 pares de oraciones con puntuación de similitud asociada\n",
    "input_pairs = [\n",
    "    ('M\\'agrada el futbol', 'Disfruto veient partits de futbol', 4),\n",
    "    ('El cel està despejat', 'Fa un dia bonic', 4.5),\n",
    "    ('M\\'encanta viatjar', 'Explorar nous llocs és una passió', 3.5),\n",
    "    ('Prefereixo l\\'estiu', 'No m\\'agrada el fred de l\\'hivern', 2.5),\n",
    "    ('Tinc gana', 'Què hi ha per sopar?', 2),\n",
    "    ('La música em relaxa', 'Escoltar música és una teràpia', 3),\n",
    "    ('El llibre és emocionant', 'No puc deixar de llegir-lo', 4),\n",
    "    ('M\\'agrada la pizza', 'És el meu menjar preferit', 4.5),\n",
    "    ('Estic cansat', 'Necessito fer una migdiada', 1.5),\n",
    "    ('Avui fa molta calor', 'És un dia sofocant', 3.5)\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T07:53:55.145748Z",
     "start_time": "2023-05-24T07:53:55.137897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "REMAP_EMBEDDINGS: bool = True\n",
    "USE_PRETRAINED: bool = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:37.944379Z",
     "start_time": "2023-05-24T12:40:37.941065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Datos reales\n",
    "TRAIN_DATA_FILE: str = \"/Users/salva/Downloads/train.tsv\"\n",
    "import pandas as pd\n",
    "tsv_data = pd.read_csv(TRAIN_DATA_FILE, sep='\\t', header=None, usecols=[1, 2, 3])\n",
    "input_pairs = tsv_data.values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.564558Z",
     "start_time": "2023-05-24T12:40:37.948890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(sentence_1) for sentence_1, _, _ in input_pairs]\n",
    "sentences_2_preproc = [simple_preprocess(sentence_2) for _, sentence_2, _ in input_pairs]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "diccionario = Dictionary(sentences_pairs_flattened)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.643938Z",
     "start_time": "2023-05-24T12:40:38.570613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Len: 30 28\n",
      "[0, 11, 13, 1, 9, 10, 5, 14, 8, 7, 2, 8, 12, 2, 6, 4, 3, 15]\n"
     ]
    }
   ],
   "source": [
    "print(\"Max Len:\", max([len(s) for s in sentences_1_preproc]), max([len(s) for s in sentences_2_preproc]))\n",
    "print(list(diccionario.doc2idx(sentences_1_preproc[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:40:38.648117Z",
     "start_time": "2023-05-24T12:40:38.645076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def map_word_embeddings(\n",
    "        sentence: str,\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map to word-embedding indices\n",
    "    :param sentence:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence_preproc = simple_preprocess(sentence)\n",
    "    _vectors = np.zeros(sequence_len, dtype=np.int32)\n",
    "    index = 0\n",
    "    for word in sentence_preproc:\n",
    "        if fixed_dictionary is not None:\n",
    "            if word in fixed_dictionary.token2id:\n",
    "                # Sumo 1 porque el valor 0 está reservado a padding\n",
    "                _vectors[index] = fixed_dictionary.token2id[word] + 1\n",
    "                index += 1\n",
    "        else:\n",
    "            if word in wv_model.key_to_index:\n",
    "                _vectors[index] = wv_model.key_to_index[word] + 1\n",
    "                index += 1\n",
    "    return _vectors\n",
    "\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
    "    :param sentence_pairs:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        vector1 = map_word_embeddings(sentence_1, sequence_len, fixed_dictionary)\n",
    "        vector2 = map_word_embeddings(sentence_2, sequence_len, fixed_dictionary)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:54:04.894847Z",
     "start_time": "2023-05-24T12:54:04.890375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([ 1, 12, 14,  2, 10, 11,  6, 15,  9,  8,  3,  9, 13,  3,  7,  5,  4,\n",
      "       16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=int32), array([7749,    9, 2784,    6,   15,    9,    8,    3,    9,   13,    3,\n",
      "          7,    5,    4,   16,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "      dtype=int32)), 3.5)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
    "mapped = map_pairs(input_pairs, fixed_dictionary=diccionario if REMAP_EMBEDDINGS else None)\n",
    "# for vectors, similitud in mapped:\n",
    "#     print(f\"Pares de vectores: {vectors[0].shape}, {vectors[1].shape}\")\n",
    "#     print(f\"Puntuación de similitud: {similitud}\")\n",
    "print(mapped[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:54:31.458466Z",
     "start_time": "2023-05-24T12:54:31.456474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "def build_and_compile_model(\n",
    "        input_length: int = 32,\n",
    "        hidden_size: int = 64,\n",
    "        dictionary_size: int = 1000,\n",
    "        embedding_size: int = 16,\n",
    "        pretrained_weights: Optional[np.ndarray] = None,\n",
    "        learning_rate: float = 0.001,\n",
    "        trainable: bool = False,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Este es un modelo muy básico. Hace lo mismo que el modelo single_vector. La puntuación es mejor por no eliminar stopwords.\n",
    "    :param input_length:\n",
    "    :param hidden_size:\n",
    "    :param dictionary_size:\n",
    "    :param embedding_size:\n",
    "    :param pretrained_weights:\n",
    "    :param learning_rate:\n",
    "    :param trainable:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    else:\n",
    "        dictionary_size = pretrained_weights.shape[0]\n",
    "        embedding_size = pretrained_weights.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True,\n",
    "            embeddings_initializer=initializer, trainable=trainable, )\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "    _pooled_1, _pooled_2 = pooling(_embedded_1, mask=_input_mask_1), pooling(_embedded_2, mask=_input_mask_2)\n",
    "\n",
    "    # Compute the cosine distance\n",
    "    projected_1 = tf.linalg.l2_normalize(_pooled_1, axis=1, )\n",
    "    projected_2 = tf.linalg.l2_normalize(_pooled_2, axis=1, )\n",
    "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=output,)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:08.667870Z",
     "start_time": "2023-05-24T15:30:08.659838Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "import tensorflow as tf\n",
    "def build_and_compile_model_2(\n",
    "        input_length: int = 32,\n",
    "        dictionary_size: int = 1000,\n",
    "        embedding_size: int = 16,\n",
    "        learning_rate: float = 0.001,\n",
    "        trainable: bool = False,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Este es un modelo algo más avanzado. Calcula internamente una media ponderada de los word embeddings. Calcula también la proyección.\n",
    "    :param input_length:\n",
    "    :param hidden_size:\n",
    "    :param dictionary_size:\n",
    "    :param embedding_size:\n",
    "    :param pretrained_weights:\n",
    "    :param learning_rate:\n",
    "    :param trainable:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_1, input_2 = tf.keras.Input((input_length, ), dtype=tf.int32, ), tf.keras.Input((input_length, ), dtype=tf.int32, )\n",
    "    # Define Layers\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True, )\n",
    "    else:\n",
    "        dictionary_size = pretrained_weights.shape[0]\n",
    "        embedding_size = pretrained_weights.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True,\n",
    "            embeddings_initializer=initializer, trainable=trainable, )\n",
    "    # Pass through the layers\n",
    "    _input_mask_1, _input_mask_2 = tf.not_equal(input_1, 0), tf.not_equal(input_2, 0)\n",
    "    _embedded_1, _embedded_2 = embedding(input_1, ), embedding(input_2, )\n",
    "\n",
    "    # Compute custom weights\n",
    "    weights_computation = tf.keras.layers.Dense(1, name=\"weight_computation\")\n",
    "    dropout = tf.keras.layers.Dropout(0.2, name=\"dropout_in\")\n",
    "    _weights_1 = weights_computation(dropout(_embedded_1))\n",
    "    weights_1 = tf.squeeze(_weights_1, axis=[-1])\n",
    "    _weights_2 = weights_computation(dropout(_embedded_2))\n",
    "    weights_2 = tf.squeeze(_weights_2, axis=[-1])\n",
    "    # Define softmax\n",
    "    softmax = tf.keras.layers.Softmax(name=\"weighted_sum_softmax\")\n",
    "    scores_1 = softmax(weights_1, mask=_input_mask_1)\n",
    "    _pooled_1 = tf.math.reduce_sum(_embedded_1 * tf.expand_dims(scores_1, axis=-1), axis=1)\n",
    "    scores_2 = softmax(weights_2, mask=_input_mask_2)\n",
    "    _pooled_2 = tf.math.reduce_sum(_embedded_2 * tf.expand_dims(scores_2, axis=-1) , axis=1)\n",
    "    # Compute the distance\n",
    "    dense_output = tf.keras.layers.Dense(1)\n",
    "    dropout_out = tf.keras.layers.Dropout(0.2, name=\"dropout_out\")\n",
    "    projected_1 = tf.linalg.l2_normalize(_pooled_1, axis=1, )\n",
    "    projected_2 = tf.linalg.l2_normalize(_pooled_2, axis=1, )\n",
    "    output = dense_output(dropout_out(projected_1 * projected_2), )\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2, ), outputs=output,)\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:10.672062Z",
     "start_time": "2023-05-24T15:30:10.667865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [],
   "source": [
    "# Definir constantes de entrenamiento\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64\n",
    "train_val_split: float = 0.8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:10.854703Z",
     "start_time": "2023-05-24T15:30:10.850423Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [],
   "source": [
    "# Obtener x_train e y_train\n",
    "train_slice: int = int(len(mapped) * train_val_split)\n",
    "\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
    "    :param pair_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.row_stack(_x_1), np.row_stack(_x_2)), np.array(_y)\n",
    "\n",
    "# Obtener las listas de train y test\n",
    "x_train, y_train = pair_list_to_x_y(mapped[:train_slice])\n",
    "x_val, y_val = pair_list_to_x_y(mapped[train_slice:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.041186Z",
     "start_time": "2023-05-24T15:30:11.033967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "# Preparar los conjuntos de datos de entrenamiento y validación\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.209237Z",
     "start_time": "2023-05-24T15:30:11.198383Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [],
   "source": [
    "pretrained_weights: Optional[np.ndarray] = None\n",
    "if USE_PRETRAINED:\n",
    "    if REMAP_EMBEDDINGS:\n",
    "        pretrained_weights = np.zeros(\n",
    "            (len(diccionario.token2id) + 1, wv_model.vector_size),  dtype=np.float32)\n",
    "        for token, _id in diccionario.token2id.items():\n",
    "            if token in wv_model:\n",
    "                pretrained_weights[_id + 1] = wv_model[token]\n",
    "            else:\n",
    "                # In W2V, OOV will not have a representation. We will use 0.\n",
    "                pass\n",
    "    else:\n",
    "        # Not recommended (this will consume A LOT of RAM)\n",
    "        pretrained_weights = np.zeros((wv_model.vectors.shape[0] + 1, wv_model.vector_size,),  dtype=np.float32)\n",
    "        pretrained_weights[1:, :] = wv_model.vectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.518333Z",
     "start_time": "2023-05-24T15:30:11.415410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:11.597751Z",
     "start_time": "2023-05-24T15:30:11.594305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 5.2998 - val_loss: 5.1098\n",
      "Epoch 2/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 4.6893 - val_loss: 4.8049\n",
      "Epoch 3/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.9959 - val_loss: 4.5069\n",
      "Epoch 4/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.2882 - val_loss: 4.2480\n",
      "Epoch 5/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.6491 - val_loss: 4.0402\n",
      "Epoch 6/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1103 - val_loss: 3.8774\n",
      "Epoch 7/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6681 - val_loss: 3.7486\n",
      "Epoch 8/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.3103 - val_loss: 3.6450\n",
      "Epoch 9/64\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0247 - val_loss: 3.5601\n",
      "Epoch 10/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.8007 - val_loss: 3.4898\n",
      "Epoch 11/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.6278 - val_loss: 3.4311\n",
      "Epoch 12/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4958 - val_loss: 3.3822\n",
      "Epoch 13/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3964 - val_loss: 3.3416\n",
      "Epoch 14/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3217 - val_loss: 3.3081\n",
      "Epoch 15/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2652 - val_loss: 3.2804\n",
      "Epoch 16/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2222 - val_loss: 3.2576\n",
      "Epoch 17/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1889 - val_loss: 3.2385\n",
      "Epoch 18/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1628 - val_loss: 3.2230\n",
      "Epoch 19/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1422 - val_loss: 3.2102\n",
      "Epoch 20/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 3.1995\n",
      "Epoch 21/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1127 - val_loss: 3.1902\n",
      "Epoch 22/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1018 - val_loss: 3.1817\n",
      "Epoch 23/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0933 - val_loss: 3.1752\n",
      "Epoch 24/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0859 - val_loss: 3.1686\n",
      "Epoch 25/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0804 - val_loss: 3.1632\n",
      "Epoch 26/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0751 - val_loss: 3.1587\n",
      "Epoch 27/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0713 - val_loss: 3.1535\n",
      "Epoch 28/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0679 - val_loss: 3.1514\n",
      "Epoch 29/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0652 - val_loss: 3.1464\n",
      "Epoch 30/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0622 - val_loss: 3.1456\n",
      "Epoch 31/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0601 - val_loss: 3.1416\n",
      "Epoch 32/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 3.1414\n",
      "Epoch 33/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 3.1377\n",
      "Epoch 34/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0557 - val_loss: 3.1376\n",
      "Epoch 35/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0549 - val_loss: 3.1355\n",
      "Epoch 36/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0542 - val_loss: 3.1336\n",
      "Epoch 37/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0535 - val_loss: 3.1344\n",
      "Epoch 38/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0530 - val_loss: 3.1299\n",
      "Epoch 39/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 3.1332\n",
      "Epoch 40/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0519 - val_loss: 3.1268\n",
      "Epoch 41/64\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0517 - val_loss: 3.1311\n",
      "Epoch 42/64\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.0510 - val_loss: 3.1255\n",
      "Epoch 43/64\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.0511 - val_loss: 3.1270\n",
      "Epoch 44/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 3.1266\n",
      "Epoch 45/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0507 - val_loss: 3.1209\n",
      "Epoch 46/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 3.1292\n",
      "Epoch 47/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 3.1155\n",
      "Epoch 48/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0501 - val_loss: 3.1291\n",
      "Epoch 49/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0503 - val_loss: 3.1144\n",
      "Epoch 50/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0504 - val_loss: 3.1215\n",
      "Epoch 51/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 3.1206\n",
      "Epoch 52/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 3.1096\n",
      "Epoch 53/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0507 - val_loss: 3.1291\n",
      "Epoch 54/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0516 - val_loss: 3.1020\n",
      "Epoch 55/64\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0515 - val_loss: 3.1265\n",
      "Epoch 56/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0521 - val_loss: 3.1060\n",
      "Epoch 57/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 3.1133\n",
      "Epoch 58/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0513 - val_loss: 3.1177\n",
      "Epoch 59/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0515 - val_loss: 3.0985\n",
      "Epoch 60/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0512 - val_loss: 3.1243\n",
      "Epoch 61/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0526 - val_loss: 3.0945\n",
      "Epoch 62/64\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.0512 - val_loss: 3.1209\n",
      "Epoch 63/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0517 - val_loss: 3.0975\n",
      "Epoch 64/64\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.0513 - val_loss: 3.1082\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x4a57d5f60>"
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir y compilar el modelo\n",
    "model = build_and_compile_model(pretrained_weights=pretrained_weights, )\n",
    "# Entrenar el modelo\n",
    "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:26.688759Z",
     "start_time": "2023-05-24T15:30:11.768236Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 860us/step\n",
      "Correlación de Pearson: 0.3388412697357464\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred = model.predict(x_val)\n",
    "# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlación de Pearson\n",
    "print(f\"Correlación de Pearson: {correlation}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:27.726622Z",
     "start_time": "2023-05-24T15:30:27.655446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [
    {
     "data": {
      "text/plain": "[(4.1745477, 3.5),\n (3.508358, 1.25),\n (4.7923083, 3.67),\n (3.498742, 2.25),\n (3.641457, 2.0),\n (4.62667, 2.75),\n (3.8159537, 2.67),\n (4.4331594, 2.5),\n (4.2778196, 2.5),\n (3.7935877, 3.0),\n (4.0941725, 3.0),\n (4.51306, 1.0),\n (3.8270597, 2.0),\n (4.810272, 4.0),\n (4.8919096, 3.0),\n (4.757413, 2.75),\n (4.5241733, 2.0),\n (3.3888803, 3.0),\n (3.80712, 2.0),\n (4.670128, 3.75),\n (4.272671, 2.75),\n (4.6512895, 1.67),\n (3.3618736, 4.5),\n (3.7664127, 1.75),\n (3.8170853, 2.75),\n (3.674805, 2.25),\n (4.124276, 2.0),\n (3.881369, 2.5),\n (4.1059375, 2.25),\n (4.4007483, 1.0),\n (4.9323688, 4.67),\n (3.162551, 2.5),\n (4.130235, 3.0),\n (4.3740788, 3.0),\n (4.1455393, 3.25),\n (3.3173716, 2.5),\n (4.459638, 2.0),\n (4.979721, 1.75),\n (5.0, 3.5),\n (2.556395, 3.0),\n (4.541895, 1.75),\n (4.158573, 1.75),\n (3.4692552, 2.25),\n (4.553137, 5.0),\n (4.8470793, 3.0),\n (4.3951464, 2.75),\n (4.4824486, 1.5),\n (4.3023868, 2.25),\n (4.0057344, 2.0),\n (4.125059, 2.0),\n (5.0, 2.0),\n (4.357737, 1.5),\n (4.726922, 3.67),\n (3.299096, 2.0),\n (4.182562, 3.0),\n (4.8275366, 3.0),\n (4.734172, 3.0),\n (4.6331787, 1.0),\n (4.430508, 1.75),\n (2.8568113, 1.5),\n (3.2611692, 3.0),\n (4.6177626, 1.25),\n (4.2225704, 1.75),\n (3.9658618, 1.5),\n (4.4617944, 3.0),\n (4.5109015, 3.0),\n (4.254449, 3.5),\n (4.2203546, 2.0),\n (4.5333095, 2.0),\n (3.8816454, 3.0),\n (4.226392, 2.0),\n (4.2548947, 1.5),\n (3.95288, 4.0),\n (4.680888, 1.25),\n (4.2105093, 2.0),\n (3.6408627, 5.0),\n (3.2221615, 4.0),\n (4.5249825, 3.5),\n (4.6204796, 1.0),\n (3.122505, 0.0),\n (3.677131, 3.0),\n (4.442162, 2.25),\n (3.2730124, 2.75),\n (4.4836235, 2.75),\n (4.7408533, 3.25),\n (4.4643254, 1.5),\n (4.1516695, 3.5),\n (3.9256582, 2.75),\n (2.4673002, 2.0),\n (4.2569323, 3.0),\n (4.393168, 2.5),\n (3.5568566, 1.5),\n (4.105109, 2.5),\n (4.5639987, 2.75),\n (4.5495405, 1.33),\n (4.5715647, 4.75),\n (3.3322418, 2.0),\n (3.0799265, 2.25),\n (4.3596787, 4.0),\n (4.123072, 1.5),\n (2.5358107, 2.5),\n (3.9833019, 1.75),\n (3.7811356, 1.5),\n (4.190261, 2.75),\n (3.8825583, 5.0),\n (4.2763414, 3.0),\n (3.710062, 2.5),\n (4.9216275, 3.0),\n (4.5422325, 2.75),\n (4.412313, 5.0),\n (4.270701, 2.75),\n (4.4021683, 2.25),\n (4.0880065, 3.25),\n (4.243764, 2.25),\n (4.132914, 3.0),\n (3.86405, 2.5),\n (3.7691715, 2.75),\n (3.894832, 3.25),\n (4.0630455, 3.25),\n (4.679835, 2.5),\n (3.5114021, 3.0),\n (4.6442366, 1.25),\n (3.9164808, 3.0),\n (3.9413924, 2.0),\n (4.539327, 2.25),\n (4.527543, 2.75),\n (4.208976, 3.0),\n (3.8759136, 3.0),\n (4.3088393, 2.75),\n (4.6935124, 3.0),\n (4.5597334, 1.75),\n (4.4234724, 3.0),\n (3.4058623, 1.33),\n (3.6274028, 0.33),\n (3.4135823, 1.75),\n (4.1680984, 2.75),\n (4.2979364, 5.0),\n (2.645254, 2.5),\n (3.7155118, 2.0),\n (4.388427, 2.75),\n (4.137681, 2.5),\n (4.2262836, 1.75),\n (4.1945786, 2.0),\n (4.828778, 2.5),\n (4.848978, 2.25),\n (3.67307, 1.0),\n (3.9418454, 4.25),\n (3.916054, 3.0),\n (4.3753614, 2.75),\n (3.640827, 2.75),\n (4.310586, 2.67),\n (3.8827124, 1.5),\n (4.7507496, 3.5),\n (3.9398031, 1.25),\n (4.037691, 2.75),\n (3.7822666, 2.75),\n (3.4298034, 3.0),\n (4.433617, 2.67),\n (4.2161837, 3.0),\n (4.3688293, 3.0),\n (4.413768, 3.0),\n (4.5884533, 4.0),\n (3.8502626, 5.0),\n (4.1802764, 3.0),\n (3.9070165, 1.75),\n (4.347114, 3.5),\n (4.2067366, 3.0),\n (4.4315658, 2.25),\n (5.0, 4.67),\n (4.0007463, 3.0),\n (3.7894666, 3.0),\n (3.7433, 2.75),\n (3.6038861, 1.0),\n (4.255288, 2.75),\n (4.5885825, 2.67),\n (3.8817499, 3.0),\n (3.994748, 3.0),\n (2.9343355, 3.0),\n (3.8330216, 2.0),\n (3.039631, 5.0),\n (4.0208416, 1.75),\n (4.072621, 1.75),\n (3.7792077, 1.5),\n (4.157881, 2.67),\n (4.765972, 1.33),\n (3.9573479, 3.0),\n (4.2725873, 1.0),\n (4.4745216, 3.0),\n (3.7933002, 3.0),\n (4.858281, 2.75),\n (4.517109, 2.0),\n (4.1751904, 3.5),\n (4.586012, 2.75),\n (4.153051, 3.0),\n (3.311334, 1.25),\n (3.5927558, 1.75),\n (3.537369, 2.75),\n (3.5070517, 2.25),\n (4.7159133, 3.75),\n (4.3166475, 3.0),\n (3.245132, 1.5),\n (4.469926, 2.25),\n (4.195001, 2.75),\n (4.2204266, 2.5),\n (3.957282, 3.0),\n (4.2196465, 2.5),\n (4.1939883, 1.75),\n (4.353512, 3.5),\n (3.5729764, 1.5),\n (3.5676675, 3.0),\n (3.6297438, 1.75),\n (3.9305406, 3.0),\n (4.551254, 1.0),\n (4.3329306, 2.75),\n (4.0506773, 3.0),\n (4.8232226, 3.0),\n (3.1560774, 2.75),\n (4.163842, 3.0),\n (3.3781881, 2.5),\n (4.710619, 2.25),\n (4.5630054, 3.75),\n (4.894729, 1.75),\n (4.1321154, 1.5),\n (4.350924, 1.5),\n (4.6211653, 3.0),\n (2.8251052, 4.67),\n (4.2437525, 2.5),\n (4.613863, 2.5),\n (4.2849755, 3.0),\n (3.8230324, 3.0),\n (4.001067, 4.0),\n (4.768922, 2.5),\n (3.6297855, 2.75),\n (4.423204, 2.5),\n (3.7133048, 3.0),\n (4.526558, 3.0),\n (0.30664608, 2.0),\n (4.745959, 4.75),\n (4.6738815, 4.0),\n (4.8483377, 1.5),\n (4.9973326, 1.75),\n (4.4515114, 1.75),\n (3.4546766, 1.75),\n (4.2663603, 1.25),\n (3.8727431, 3.0),\n (3.8109744, 3.0),\n (5.0, 1.0),\n (4.2915926, 2.75),\n (4.4890995, 2.0),\n (4.8543987, 3.0),\n (4.0821924, 2.25),\n (4.347559, 1.5),\n (4.3034773, 1.0),\n (3.8172257, 3.25),\n (4.1679425, 1.75),\n (4.42504, 3.25),\n (4.5616107, 3.0),\n (3.5883796, 3.0),\n (4.3705373, 1.0),\n (4.5451174, 3.0),\n (4.604561, 3.0),\n (4.56168, 3.75),\n (4.3600535, 2.0),\n (2.8061602, 2.5),\n (4.443696, 1.5),\n (3.7860136, 2.0),\n (4.4149847, 2.5),\n (4.8750906, 4.5),\n (4.0088677, 1.75),\n (4.136531, 2.0),\n (3.7806773, 2.5),\n (3.5702336, 3.0),\n (4.105701, 2.75),\n (4.659761, 2.75),\n (4.50657, 4.5),\n (4.9415193, 2.0),\n (4.22198, 2.67),\n (3.773405, 1.75),\n (4.399582, 2.75),\n (4.340646, 0.0),\n (4.480205, 2.0),\n (4.140929, 2.5),\n (3.4653997, 2.5),\n (4.8097253, 2.0),\n (4.317318, 2.67),\n (3.5254269, 3.0),\n (4.4737597, 2.0),\n (4.845868, 2.75),\n (4.1026793, 4.0),\n (4.959462, 2.75),\n (4.558802, 2.75),\n (4.2413006, 2.25),\n (4.407235, 2.0),\n (2.9441938, 0.0),\n (3.5332913, 2.0),\n (3.8681011, 1.0),\n (4.166227, 3.0),\n (4.6853304, 1.25),\n (4.0487356, 3.0),\n (3.796736, 3.0),\n (4.0418253, 2.0),\n (3.7638938, 3.25),\n (3.0270565, 3.0),\n (3.1730227, 4.75),\n (4.979296, 3.0),\n (2.820644, 2.5),\n (3.8050318, 2.5),\n (4.478393, 1.25),\n (3.627616, 2.0),\n (3.856854, 3.0),\n (3.8735352, 2.75),\n (4.4483805, 1.25),\n (2.6227067, 2.0),\n (4.548352, 1.33),\n (4.12726, 3.0),\n (4.1495886, 3.0),\n (4.633379, 3.5),\n (5.0000005, 2.0),\n (3.8717775, 3.0),\n (3.9762366, 1.75),\n (4.5187755, 2.75),\n (4.1056705, 2.5),\n (4.2361264, 2.33),\n (4.7322464, 1.25),\n (4.6868906, 2.25),\n (4.7858453, 3.0),\n (3.7204936, 2.0),\n (4.468545, 1.0),\n (3.3579235, 1.75),\n (3.665617, 2.75),\n (4.59686, 3.0),\n (4.724625, 2.0),\n (4.0688553, 2.0),\n (4.663595, 1.5),\n (3.6694503, 3.0),\n (4.5503845, 1.0),\n (4.4275045, 2.0),\n (3.5940564, 2.33),\n (2.0500586, 1.5),\n (3.7383862, 3.0),\n (4.3873243, 2.0),\n (4.827145, 3.0),\n (4.8977857, 1.0),\n (3.7072988, 3.0),\n (4.1987042, 2.75),\n (4.740611, 2.0),\n (4.177864, 2.67),\n (4.0284843, 2.75),\n (4.944363, 3.5),\n (4.2587843, 5.0),\n (2.9726765, 3.0),\n (4.6276574, 3.0),\n (3.98951, 1.25),\n (3.2894683, 1.5),\n (3.5134363, 2.0),\n (3.9940886, 3.25),\n (4.0120735, 3.25),\n (4.385725, 2.75),\n (4.049874, 4.25),\n (4.309268, 1.0),\n (4.9268227, 2.75),\n (3.8384783, 2.5),\n (4.64824, 2.25),\n (4.123086, 1.25),\n (3.9972491, 2.67),\n (3.9629674, 2.75),\n (4.729576, 3.5),\n (2.931435, 2.75),\n (3.7596262, 3.0),\n (4.3276877, 2.0),\n (4.5077004, 2.5),\n (4.0391893, 1.75),\n (4.924257, 1.33),\n (4.245408, 3.0),\n (3.7892132, 2.67),\n (4.2937202, 1.25),\n (3.9852035, 2.75),\n (3.952814, 2.0),\n (4.3907576, 2.5),\n (4.5661187, 1.5),\n (4.789385, 3.0),\n (4.331649, 2.33),\n (3.7153525, 4.75),\n (3.447546, 3.0),\n (2.7790713, 3.0),\n (4.7464857, 2.5),\n (3.861586, 3.5),\n (4.329769, 2.0),\n (3.7802248, 3.0),\n (4.021925, 2.5),\n (3.505291, 2.0),\n (4.648778, 3.0),\n (3.4683104, 2.0),\n (2.7660182, 2.0),\n (3.728378, 2.5),\n (4.828375, 3.0),\n (4.77763, 1.5),\n (3.910554, 2.75),\n (4.372597, 1.25),\n (4.3581514, 3.0),\n (4.8035526, 3.0),\n (3.8342345, 2.75),\n (4.182008, 2.5),\n (4.510083, 1.5),\n (4.719755, 3.0),\n (3.4797003, 1.0),\n (3.8504412, 1.5),\n (4.1059227, 1.0),\n (4.116646, 2.25),\n (4.4205585, 2.0),\n (3.7616525, 2.75),\n (4.6803946, 2.75),\n (4.990463, 2.75),\n (4.8092957, 3.0),\n (3.4705954, 1.5)]"
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(y_pred, y_train))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:30:34.411112Z",
     "start_time": "2023-05-24T15:30:34.409004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T15:20:48.758153Z",
     "start_time": "2023-05-24T15:20:48.718750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
